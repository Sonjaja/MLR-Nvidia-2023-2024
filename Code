# Load dataset
import pandas as pd
df = pd.read_csv('Sept 2023 - 2024 Nvidia price.csv')
df.head()

# Sorting Date column to ascending order to avoid forward bias
df['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%y')
df = df.sort_values(by='Date', ascending=True)

# Creating RSI values
# Calculate daily price changes
df['Change'] = df['Close'].diff()

# Separate gains and losses
df['Gain'] = df['Change'].apply(lambda x: x if x > 0 else 0)
df['Loss'] = df['Change'].apply(lambda x: -x if x < 0 else 0)

period = 14
df['Avg_Gain'] = df['Gain'].ewm(span=period, adjust=False).mean()
df['Avg_Loss'] = df['Loss'].ewm(span=period, adjust=False).mean()

# Calculate RS and RSI
df['RS'] = df['Avg_Gain'] / df['Avg_Loss']
df['RSI'] = 100 - (100 / (1 + df['RS']))

# Display the DataFrame with RSI
print(df[['Close', 'RSI']])

# Calculate SMA
period = 14
df['SMA'] = df['Close'].rolling(window=period, min_periods=1).mean()

df['Returns'] = df['SMA'].pct_change()
df['Returns'] = df['Returns'] * -1
df['Returns'] = df['Returns'].fillna(0)
df['Returns'] = df['Returns'].ffill()

# Display the DataFrame with SMA
print(df[['Close', 'SMA']])

# Making Bollinger Band indicator
df['Middle_Band'] = df['Close'].rolling(window=20).mean()
df['Standard_Deviation'] = df['Close'].rolling(window=20).std()
df['Upper_Band'] = df['Middle_Band'] + (2 * df['Standard_Deviation'])
df['Lower_Band'] = df['Middle_Band'] - (2 * df['Standard_Deviation'])

df['Returns'].unique()

#data cleaning
df.isnull().sum()
# Removing the rows with missing values.
df.dropna(inplace=True)
df.isnull().sum()

# Making sure all the values are numerical values except the Date Column.
print(df.dtypes)

# Import rest of the libraries
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from scipy import stats
from sklearn import metrics

# Assessing correlations and multicollinearity among all variables to evaluate their suitability for Multiple Linear Regression (MLR)
X1 = np.array(df['Volume'])
X2 = np.array(df['PE_Ratio'])
X3 = np.array(df['Revenue'])
X4 = np.array(df['RSI'])
X5 = np.array(df['SMA'])
X6 = np.array(df['Returns'])
X7 = np.array(df['Lower_Band'])
X8 = np.array(df['Middle_Band'])
X9 = np.array(df['Upper_Band'])

y = np.array(df['Close'])

X_list = [X1, X2, X3, X4, X5, X6, X7, X8, X9]
labels = ['Volume','PE_Ratio','Revenue','RSI','SMA', 'Returns', 'Lower_Band', 'Middle_Band', 'Upper_Band']
results = {}

# Creating a function to calculate features' Slope, intercept, R, P, and standard error values.
for i, X in enumerate(X_list):
    slope, intercept, r_value, p_value, std_err = stats.linregress(X, y)
    results[labels[i]] = {'slope': slope, 'intercept': intercept, 'r': r_value, 'p': p_value, 'std_err': std_err}

# Making the features appear as observations
results_df = pd.DataFrame(results).T
results_df

data = pd.DataFrame({
    'Volume': X1,
    'PE_Ratio': X2,
    'Revenue': X3,
    'RSI': X4,
    'SMA': X5,
    'Returns': X6,
    'Lower_Band': X7,
    'Middle_Band': X8,
    'Upper_Band': X9
})
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Min-Max Scaling
min_max_scaler = MinMaxScaler()
df_min_max_scaled = pd.DataFrame(min_max_scaler.fit_transform(data), columns=data.columns)

# Standardization
standard_scaler = StandardScaler()
df_standardized = pd.DataFrame(standard_scaler.fit_transform(data), columns=data.columns)

correlation_matrix = df_standardized.corr()
plt.figure(figsize=(10, 8))

sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True, cbar_kws={"shrink": .8})

df_standardized = df_standardized.fillna(0)
X = df_standardized
y = df[['Close']]

# Reshape y to be a 2D array
y = y.values.reshape(-1, 1)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the y-values
y_standardized = scaler.fit_transform(y)

# Splitting dataset into training and testing sets
train_size = int(len(data) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y_standardized[:train_size], y_standardized[train_size:]

# Train the model
model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

# Plotting the MLR
plt.scatter(y_test, y_pred)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red')
plt.show()

from sklearn.metrics import r2_score
# Calculate R-squared
r_squared = r2_score(y_test, y_pred)
print('R-squared:', r_squared)
